# RAG

document - embedding - vector - retriver 순으로 진행
prompt만으로는 부족하기 때문에 RAG를 이용(+Fine-tuning)
RAG의 구현 난이도는 어렵지 않음 (PEFT, Full Fine-tuning보다 쉬운 정도)
8단계의 세부 모델들을 플러그인 수준으로 꼈다 뺐다 하는 수준
최신정보를 보고 얘기하기 때문에 Up to Date Responses가 가장 많은 도움을 줌
답변에 관한 투명성도 상당히 높은편

# LangChain

임베더, 문서로더 등을 쉽게 가져다 쓰기 좋게 만들어줌
GPT를 활용해서 서비스를 만들때 도와주는 역할

# token

자연어처리에서 텍스트를 작은 단위로 나누어 처리하기 위해 사용되는 기본 단위
텍스트를 토큰으로 나누는 과정을 **토큰화**라고 함

## 토큰화

 - ***문자 기반 토큰화*** : 텍스트를 문자 단위로 나눔
 - ***단어 기반 토큰화*** : 텍스트를 단어 단위로 나눔
 - ***서브워드 기반 토큰화*** : 단어를 더 작은 단위(서브워드)로 나눔. ***자주 사용되는 서브워드***를 사용<br>

단어 기반 토큰화는 사전을 만들어서 예측을 하는데 한글자차이의 단어들을 다른걸로 인식하기 때문에 비효율적임 그래서 사용한게 서브워드 기반 토큰화

## 토큰의 중요성
토큰화를 잘해야 임베딩이 잘됨 <br>
LLM은 다양한 방법으로 토큰화 할 수 있으며, 어떤 방식이 사용되는에 따라 모델의 성능과 효율성이 달라질 수 있음. <br>
GPT-4o 이전 기준 토큰화 할 때 한글로 쓴 글은 토큰의 수가 영어보다 비교적 많이 나옴 (영어가 효율적임) <br>

# 컨텍스트 윈도우

# Chain 생성

## LECL
chain = prompt | model | output_parser
**|** 기호가 unix pipe 연산자와 유사함,  <br>
![image](https://github.com/user-attachments/assets/da2f5081-9eb5-4bc8-bf00-0420c7153432)




